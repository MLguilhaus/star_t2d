Building DAG of jobs...
Your conda installation is not configured to use strict channel priorities. This is however crucial for having robust and correct environments (for details, see https://conda-forge.org/docs/user/tipsandtricks.html). Please consider to configure strict priorities by executing 'conda config --set channel_priority strict'.
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=2048, mem_mib=1954, disk_mb=3456, disk_mib=3296
Select jobs to execute...

[Thu Feb  8 14:04:43 2024]
rule fastp:
    input: data/fastq/raw/SRR15881899_1.fastq.gz, data/fastq/raw/SRR15881899_2.fastq.gz
    output: data/fastq/trimmed/SRR15881899_1.fastq.gz, data/fastq/trimmed/SRR15881899_2.fastq.gz, output/fastp/SRR15881899_fastp.html, output/fastp/SRR15881899_fastp.json
    log: workflow/logs/fastp/SRR15881899.log
    jobid: 0
    reason: Forced execution
    wildcards: accession=SRR15881899
    threads: 4
    resources: mem_mb=2048, mem_mib=1954, disk_mb=3456, disk_mib=3296, tmpdir=/mnt/tmp_local/lls_1903486, runtime=120


        fastp             --thread 4             --n_base_limit 1             --average_qual 30             --length_required 50             --overrepresentation_analysis             -i data/fastq/raw/SRR15881899_1.fastq.gz             -I data/fastq/raw/SRR15881899_2.fastq.gz             -o data/fastq/trimmed/SRR15881899_1.fastq.gz             -O data/fastq/trimmed/SRR15881899_2.fastq.gz             -h output/fastp/SRR15881899_fastp.html             -j output/fastp/SRR15881899_fastp.json &> workflow/logs/fastp/SRR15881899.log
        
Activating conda environment: ../../envs/1fad0e7e02c206763c147f4a9de2ddc8_
/usr/bin/bash: line 1: 1913465 Killed                  fastp --thread 4 --n_base_limit 1 --average_qual 30 --length_required 50 --overrepresentation_analysis -i data/fastq/raw/SRR15881899_1.fastq.gz -I data/fastq/raw/SRR15881899_2.fastq.gz -o data/fastq/trimmed/SRR15881899_1.fastq.gz -O data/fastq/trimmed/SRR15881899_2.fastq.gz -h output/fastp/SRR15881899_fastp.html -j output/fastp/SRR15881899_fastp.json &> workflow/logs/fastp/SRR15881899.log
[Thu Feb  8 14:04:58 2024]
Error in rule fastp:
    jobid: 0
    input: data/fastq/raw/SRR15881899_1.fastq.gz, data/fastq/raw/SRR15881899_2.fastq.gz
    output: data/fastq/trimmed/SRR15881899_1.fastq.gz, data/fastq/trimmed/SRR15881899_2.fastq.gz, output/fastp/SRR15881899_fastp.html, output/fastp/SRR15881899_fastp.json
    log: workflow/logs/fastp/SRR15881899.log (check log file(s) for error details)
    conda-env: /hpcfs/users/a1627307/envs/1fad0e7e02c206763c147f4a9de2ddc8_
    shell:
        
        fastp             --thread 4             --n_base_limit 1             --average_qual 30             --length_required 50             --overrepresentation_analysis             -i data/fastq/raw/SRR15881899_1.fastq.gz             -I data/fastq/raw/SRR15881899_2.fastq.gz             -o data/fastq/trimmed/SRR15881899_1.fastq.gz             -O data/fastq/trimmed/SRR15881899_2.fastq.gz             -h output/fastp/SRR15881899_fastp.html             -j output/fastp/SRR15881899_fastp.json &> workflow/logs/fastp/SRR15881899.log
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job fastp since they might be corrupted:
data/fastq/trimmed/SRR15881899_1.fastq.gz, data/fastq/trimmed/SRR15881899_2.fastq.gz
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1903486.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
